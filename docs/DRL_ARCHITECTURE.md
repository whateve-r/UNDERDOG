# Arquitectura DRL para UNDERDOG - Prop Firm Trading

## üìã Executive Summary

Basado en el an√°lisis de papers cient√≠ficos (arXiv:2510.04952v2, arXiv:2510.10526v1, arXiv:2510.03236v1), implementaremos una arquitectura de **Deep Reinforcement Learning (DRL)** con gesti√≥n de riesgo expl√≠cita para cumplir con restricciones de Prop Firms (FTMO).

**Objetivo:** Maximizar retorno ajustado al riesgo mientras **garantizamos** cumplimiento de l√≠mites (Daily DD <5%, Total DD <10%).

---

## üèóÔ∏è Arquitectura de 4 Capas

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ CAPA 1: INGESTA DE DATOS                                    ‚îÇ
‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îÇ
‚îÇ ‚îÇ  OHLCV   ‚îÇ  ‚îÇTechnical ‚îÇ  ‚îÇ LLM/NLP  ‚îÇ  ‚îÇ  FRED    ‚îÇ    ‚îÇ
‚îÇ ‚îÇ  MT5/    ‚îÇ  ‚îÇIndicators‚îÇ  ‚îÇSentiment ‚îÇ  ‚îÇ  Macro   ‚îÇ    ‚îÇ
‚îÇ ‚îÇyfinance  ‚îÇ  ‚îÇ (TA-Lib) ‚îÇ  ‚îÇ (FinGPT) ‚îÇ  ‚îÇIndicators‚îÇ    ‚îÇ
‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                            ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ CAPA 2: REGIME-SWITCHING (Clasificaci√≥n de Mercado)         ‚îÇ
‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îÇ
‚îÇ ‚îÇ GMM + XGBoost Classifier                            ‚îÇ    ‚îÇ
‚îÇ ‚îÇ Output: R√©gimen actual (Trend/Range/Transition)    ‚îÇ    ‚îÇ
‚îÇ ‚îÇ Features: Volatilidad, Volumen, Indicadores        ‚îÇ    ‚îÇ
‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ
‚îÇ Paper: arXiv:2510.03236v1                                  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                            ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ CAPA 3: DRL AGENT (Decisi√≥n de Trading)                     ‚îÇ
‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îÇ
‚îÇ ‚îÇ TD3 / PPO Agent (stable-baselines3)                 ‚îÇ    ‚îÇ
‚îÇ ‚îÇ State: [Price, Indicators, Sentiment, Regime]       ‚îÇ    ‚îÇ
‚îÇ ‚îÇ Action: [Position_Size, Entry/Exit]                 ‚îÇ    ‚îÇ
‚îÇ ‚îÇ Reward: Sharpe Ratio (risk-adjusted return)        ‚îÇ    ‚îÇ
‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ
‚îÇ Paper: arXiv:2510.10526v1 (LLM + RL Integration)          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                            ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ CAPA 4: CONSTRAINED EXECUTION (Shield Module)               ‚îÇ
‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îÇ
‚îÇ ‚îÇ Safety Shield (Pre-execution Validation)            ‚îÇ    ‚îÇ
‚îÇ ‚îÇ Checks: Daily DD <5%, Total DD <10%, Max Positions  ‚îÇ    ‚îÇ
‚îÇ ‚îÇ Action: Project unsafe actions ‚Üí safe actions       ‚îÇ    ‚îÇ
‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ
‚îÇ Paper: arXiv:2510.04952v2 (Safe Trade Execution)          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                            ‚Üì
                    [MT5 Broker Execution]
```

---

## üì¶ Componentes Python

### 1. Regime-Switching Module

**Path:** `underdog/ml/regime_classifier.py`

```python
# Clasificador de R√©gimen de Mercado
# Paper: arXiv:2510.03236v1

from sklearn.mixture import GaussianMixture
from xgboost import XGBClassifier

class RegimeSwitchingModel:
    """
    Classifies market regime: Trend/Range/Transition
    
    Features:
    - ATR (volatility)
    - Volume normalized
    - ADX (trend strength)
    - Bollinger Band width
    """
    
    def __init__(self, n_regimes=3):
        self.gmm = GaussianMixture(n_components=n_regimes)
        self.classifier = XGBClassifier()
    
    def fit(self, features: np.ndarray):
        # GMM para clustering suave
        # XGBoost para clasificaci√≥n final
        pass
    
    def predict(self, features: np.ndarray) -> str:
        # Returns: 'trend' / 'range' / 'transition'
        pass
```

**Uso:**
- Entrena offline con 5 a√±os de datos hist√≥ricos
- Inference en tiempo real (cada tick/barra)
- Output usado como feature para DRL Agent

---

### 2. DRL Agent (TD3 / PPO)

**Path:** `underdog/ml/drl_agent.py`

```python
# Deep Reinforcement Learning Agent
# Paper: arXiv:2510.10526v1 (LLM + RL)

from stable_baselines3 import TD3, PPO
from stable_baselines3.common.vec_env import DummyVecEnv

class TradingDRLAgent:
    """
    TD3 Agent for adaptive signal integration
    
    State Space:
    - Price features (OHLC, indicators)
    - LLM sentiment score (-1 to 1)
    - Regime classification (one-hot)
    - Position state (size, PnL, duration)
    
    Action Space:
    - Continuous: position_size (-1 to 1)
    - Discrete: entry/exit decision
    
    Reward:
    - Sharpe Ratio (rolling 30 periods)
    - Penalized by drawdown violations
    """
    
    def __init__(self, env, algorithm='td3'):
        if algorithm == 'td3':
            self.model = TD3('MlpPolicy', env)
        elif algorithm == 'ppo':
            self.model = PPO('MlpPolicy', env)
    
    def train(self, total_timesteps=100_000):
        self.model.learn(total_timesteps=total_timesteps)
    
    def predict(self, state):
        action, _states = self.model.predict(state)
        return action
```

**Librer√≠as:**
- `stable-baselines3` (PPO, TD3)
- `gymnasium` (trading environment)

---

### 3. Safety Shield (Constrained RL)

**Path:** `underdog/execution/safety_shield.py`

```python
# Safety Shield for Prop Firm Compliance
# Paper: arXiv:2510.04952v2 (Safe Execution)

class PropFirmSafetyShield:
    """
    Pre-execution validation layer
    
    Enforces:
    - Daily Drawdown < 5% (FTMO Phase 1)
    - Total Drawdown < 10%
    - Max open positions <= 2
    - Min holding time >= 3 minutes
    
    If action violates constraint:
    ‚Üí Project to nearest safe action
    ‚Üí Log violation attempt
    ‚Üí Alert monitoring
    """
    
    def __init__(self, config: dict):
        self.max_daily_dd = config.get('max_daily_dd', 0.05)
        self.max_total_dd = config.get('max_total_dd', 0.10)
        self.max_positions = config.get('max_positions', 2)
    
    def validate_action(self, action: dict, account_state: dict) -> tuple[bool, dict]:
        """
        Returns: (is_safe, corrected_action)
        """
        # Check Daily DD
        if account_state['daily_dd_pct'] >= self.max_daily_dd:
            return False, {'action': 'close_all', 'reason': 'daily_dd_breach'}
        
        # Check Total DD
        if account_state['total_dd_pct'] >= self.max_total_dd:
            return False, {'action': 'close_all', 'reason': 'total_dd_breach'}
        
        # Check Max Positions
        if len(account_state['open_positions']) >= self.max_positions:
            if action['type'] == 'open':
                return False, {'action': 'wait', 'reason': 'max_positions'}
        
        # Reduce position size if necessary
        max_risk_per_trade = 0.015  # 1.5%
        if action.get('lot_size', 0) * action.get('risk_pct', 0) > max_risk_per_trade:
            action['lot_size'] = max_risk_per_trade / action['risk_pct']
        
        return True, action
```

**Integraci√≥n con Mt5Connector:**
```python
# En underdog/execution/mt5_executor.py
shield = PropFirmSafetyShield(config)

def execute_order(self, order: dict):
    is_safe, corrected_order = shield.validate_action(order, self.get_account_state())
    
    if not is_safe:
        logger.warning(f"Shield blocked order: {corrected_order['reason']}")
        return corrected_order
    
    # Proceed with execution
    return self.connector.place_order(corrected_order)
```

---

### 4. LLM Sentiment Integration

**Path:** `underdog/ml/sentiment_analyzer.py`

```python
# LLM Sentiment Analysis
# Paper: arXiv:2510.10526v1

from transformers import pipeline

class FinGPTSentimentAnalyzer:
    """
    Fine-tuned FinGPT for Forex sentiment
    
    Sources:
    - Reddit (r/forex, r/wallstreetbets)
    - Twitter/X financial accounts
    - News APIs (Alpha Vantage, NewsAPI)
    
    Output: Sentiment score [-1, 1]
    """
    
    def __init__(self, model_name='ProsusAI/finbert'):
        self.sentiment_pipeline = pipeline(
            "sentiment-analysis",
            model=model_name,
            device=0  # GPU
        )
    
    def analyze(self, text: str) -> float:
        """
        Returns: score in [-1, 1]
        -1 = bearish, 0 = neutral, 1 = bullish
        """
        result = self.sentiment_pipeline(text)[0]
        
        # Map to [-1, 1]
        if result['label'] == 'positive':
            return result['score']
        elif result['label'] == 'negative':
            return -result['score']
        else:
            return 0.0
    
    def get_market_sentiment(self, symbol: str, window_hours=24) -> float:
        """
        Aggregate sentiment from multiple sources
        """
        # Fetch recent news/reddit posts
        # Average sentiment scores
        pass
```

---

## üîÑ Workflow Completo

### Training Phase (Offline)

```python
# scripts/train_drl_agent.py

from underdog.ml.regime_classifier import RegimeSwitchingModel
from underdog.ml.drl_agent import TradingDRLAgent
from underdog.ml.sentiment_analyzer import FinGPTSentimentAnalyzer

# 1. Prepare historical data (5 years)
df = load_historical_data('EURUSD', '2020-01-01', '2025-01-01')

# 2. Train Regime Classifier
regime_model = RegimeSwitchingModel(n_regimes=3)
regime_model.fit(df[['atr', 'volume', 'adx', 'bb_width']])
regime_model.save('models/regime_classifier.pkl')

# 3. Create Trading Environment
env = TradingEnvironment(
    data=df,
    regime_model=regime_model,
    sentiment_analyzer=FinGPTSentimentAnalyzer(),
    safety_shield=PropFirmSafetyShield(config)
)

# 4. Train DRL Agent
agent = TradingDRLAgent(env, algorithm='td3')
agent.train(total_timesteps=500_000)  # ~1 semana GPU
agent.save('models/drl_agent_td3.zip')

# 5. Backtest with Shield
backtest_results = backtest_with_shield(agent, env, test_data)
print(f"Sharpe: {backtest_results['sharpe']}")
print(f"Max DD: {backtest_results['max_dd']}")
print(f"Shield Interventions: {backtest_results['shield_blocks']}")
```

### Production Phase (Live)

```python
# scripts/start_live_drl_trading.py

from underdog.execution.mt5_executor import Mt5Executor
from underdog.ml.drl_agent import TradingDRLAgent
from underdog.execution.safety_shield import PropFirmSafetyShield

# Load trained models
agent = TradingDRLAgent.load('models/drl_agent_td3.zip')
regime_model = RegimeSwitchingModel.load('models/regime_classifier.pkl')
shield = PropFirmSafetyShield(config)

# Initialize executor
executor = Mt5Executor(shield=shield)

# Main loop
while True:
    # 1. Get current state
    market_data = executor.get_market_data('EURUSD')
    sentiment = sentiment_analyzer.get_market_sentiment('EURUSD')
    regime = regime_model.predict(market_data)
    
    state = create_state_vector(market_data, sentiment, regime)
    
    # 2. DRL Agent decision
    action = agent.predict(state)
    
    # 3. Shield validation
    is_safe, corrected_action = shield.validate_action(
        action, 
        executor.get_account_state()
    )
    
    # 4. Execute (if safe)
    if is_safe:
        executor.execute_order(corrected_action)
    else:
        logger.warning(f"Shield blocked: {corrected_action['reason']}")
    
    time.sleep(60)  # Check every minute
```

---

## üìä Estructura de Carpetas Actualizada

```
underdog/
‚îú‚îÄ‚îÄ ml/                          # NEW: Machine Learning components
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ regime_classifier.py     # Regime-Switching (GMM+XGBoost)
‚îÇ   ‚îú‚îÄ‚îÄ drl_agent.py             # TD3/PPO Agent (stable-baselines3)
‚îÇ   ‚îú‚îÄ‚îÄ sentiment_analyzer.py    # FinGPT/FinBERT Sentiment
‚îÇ   ‚îú‚îÄ‚îÄ trading_env.py           # Gymnasium Trading Environment
‚îÇ   ‚îî‚îÄ‚îÄ feature_engineering.py   # Feature extraction
‚îÇ
‚îú‚îÄ‚îÄ execution/
‚îÇ   ‚îú‚îÄ‚îÄ safety_shield.py         # NEW: Constrained RL Shield
‚îÇ   ‚îú‚îÄ‚îÄ mt5_executor.py          # UPDATED: Integrate Shield
‚îÇ   ‚îî‚îÄ‚îÄ recovery.py
‚îÇ
‚îú‚îÄ‚îÄ risk/                        # CONSOLIDATED
‚îÇ   ‚îú‚îÄ‚îÄ prop_firm_rme.py         # FTMO limits
‚îÇ   ‚îú‚îÄ‚îÄ position_sizing.py
‚îÇ   ‚îî‚îÄ‚îÄ drawdown_monitor.py
‚îÇ
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ mt5_historical_loader.py
‚îÇ   ‚îú‚îÄ‚îÄ reddit_scraper.py        # NEW: Reddit sentiment
‚îÇ   ‚îî‚îÄ‚îÄ news_aggregator.py       # NEW: News + Twitter
‚îÇ
‚îî‚îÄ‚îÄ backtesting/
    ‚îú‚îÄ‚îÄ bt_engine.py
    ‚îî‚îÄ‚îÄ drl_backtester.py        # NEW: Backtest DRL agents

scripts/
‚îú‚îÄ‚îÄ train_drl_agent.py           # NEW: Offline training
‚îú‚îÄ‚îÄ start_live_drl_trading.py    # NEW: Live DRL trading
‚îî‚îÄ‚îÄ backtest_drl_strategy.py     # NEW: DRL backtest

models/                          # NEW: Trained models
‚îú‚îÄ‚îÄ regime_classifier.pkl
‚îú‚îÄ‚îÄ drl_agent_td3.zip
‚îî‚îÄ‚îÄ sentiment_analyzer/
```

---

## üöÄ Plan de Implementaci√≥n (3 Semanas)

### Semana 1: Foundation
- [ ] Setup `underdog/ml/` estructura
- [ ] Implementar `RegimeSwitchingModel` (GMM + XGBoost)
- [ ] Crear `TradingEnvironment` (Gymnasium)
- [ ] Configurar `stable-baselines3` + GPU

### Semana 2: DRL Core
- [ ] Implementar `TradingDRLAgent` (TD3)
- [ ] Integrar `FinGPTSentimentAnalyzer`
- [ ] Train offline con 5 a√±os datos (GPU)
- [ ] Backtest con m√©tricas Sharpe/DD

### Semana 3: Safety + Production
- [ ] Implementar `PropFirmSafetyShield`
- [ ] Integrar Shield en `Mt5Executor`
- [ ] Tests end-to-end con DEMO MT5
- [ ] Monitoring + alertas

---

## üìà M√©tricas de √âxito

### Training Phase
- **Sharpe Ratio > 1.5** (backtest 2024)
- **Max Drawdown < 8%** (sin shield)
- **Shield Blocks < 5%** (baja interferencia)

### Paper Trading (30 d√≠as)
- **Daily DD never > 4.5%** (margen de seguridad)
- **Total DD never > 9%**
- **Uptime > 99.9%**
- **Sharpe Ratio > 1.2** (live)

### FTMO Phase 1 (30 d√≠as)
- **Profit > 8%** (objetivo FTMO)
- **Daily DD < 5%** (hard limit)
- **Total DD < 10%** (hard limit)
- **Win Rate > 50%**

---

## üî¨ Papers de Referencia

1. **arXiv:2510.04952v2** - Safe and Compliant Trade Execution
   - Constrained RL con Shield Module
   - PPO + Safety Constraints
   
2. **arXiv:2510.10526v1** - Integrating LLM and RL for Trading
   - TD3 para adaptive signal weighting
   - FinGPT sentiment integration
   
3. **arXiv:2510.03236v1** - Regime-Switching Methods
   - GMM + XGBoost para clasificaci√≥n de r√©gimen
   - Feature engineering para volatilidad

---

## üêç Dependencias Nuevas

```toml
# pyproject.toml

[project.dependencies]
# DRL
stable-baselines3 = "^2.2.0"
gymnasium = "^0.29.0"
tensorboard = "^2.15.0"

# ML
xgboost = "^2.0.0"
scikit-learn = "^1.3.0"
hmmlearn = "^0.3.0"

# NLP/LLM
transformers = "^4.35.0"
torch = "^2.1.0"  # GPU support
sentencepiece = "^0.1.99"

# Reddit/News
praw = "^7.7.0"  # Reddit API
tweepy = "^4.14.0"  # Twitter API
newsapi-python = "^0.2.7"
```

---

## ‚ö†Ô∏è Consideraciones Cr√≠ticas

### GPU Requirements
- **Training:** RTX 3060+ (12GB VRAM) o cloud GPU
- **Inference:** CPU suficiente para live trading
- **Estimaci√≥n:** 1 semana training offline (500k timesteps)

### Complejidad vs. Simplicidad
- **Fase 1 (Actual):** Usar DRL solo para **position sizing** + **regime detection**
- **Fase 2 (Futuro):** DRL full control (entry/exit signals)
- **Raz√≥n:** Reducir superficie de error durante FTMO

### Fallback Strategy
- Si DRL falla en paper trading ‚Üí **Fallback a estrategias rule-based actuales**
- Shield es **mandatorio** independiente de DRL
- Monitoreo 24/7 con alertas Telegram

---

## üìû Pr√≥ximos Pasos Inmediatos

1. **AHORA:** ¬øAprobamos esta arquitectura?
2. **Hoy:** Crear `underdog/ml/` estructura vac√≠a
3. **Ma√±ana:** Implementar `safety_shield.py` (cr√≠tico para Prop Firm)
4. **Esta semana:** Integrar Shield en `Mt5Executor`
5. **Semana pr√≥xima:** Comenzar training DRL offline

---

**√öltima actualizaci√≥n:** 2025-01-22  
**Autor:** UNDERDOG Team  
**Status:** PROPUESTA - Pending Approval
